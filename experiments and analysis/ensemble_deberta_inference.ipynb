{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import argparse\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments\n",
    "from transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "from scipy.special import softmax\n",
    "import evaluate\n",
    "from datasets import Dataset, features\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'B-EMAIL', 1: 'B-ID_NUM', 2: 'B-NAME_STUDENT', 3: 'B-PHONE_NUM', 4: 'B-STREET_ADDRESS', 5: 'B-URL_PERSONAL', 6: 'B-USERNAME', 7: 'I-ID_NUM', 8: 'I-NAME_STUDENT', 9: 'I-PHONE_NUM', 10: 'I-STREET_ADDRESS', 11: 'I-URL_PERSONAL', 12: 'O'}\n"
     ]
    }
   ],
   "source": [
    "all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n",
    "label2id = {l: i for i,l in enumerate(all_labels)}\n",
    "id2label = {v:k for k,v in label2id.items()}\n",
    "\n",
    "target = [\n",
    "    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', \n",
    "    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', \n",
    "    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL'\n",
    "]\n",
    "\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer, label2id, max_length):\n",
    "\n",
    "    # rebuild text from tokens\n",
    "    text = []\n",
    "    labels = []\n",
    "\n",
    "    for t, l, ws in zip(\n",
    "        example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]\n",
    "    ):\n",
    "        text.append(t)\n",
    "        labels.extend([l] * len(t))\n",
    "\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            labels.append(\"O\")\n",
    "\n",
    "    # actual tokenization\n",
    "    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, max_length=max_length)\n",
    "\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    text = \"\".join(text)\n",
    "    token_labels = []\n",
    "\n",
    "    for start_idx, end_idx in tokenized.offset_mapping:\n",
    "        # CLS token\n",
    "        if start_idx == 0 and end_idx == 0:\n",
    "            token_labels.append(label2id[\"O\"])\n",
    "            continue\n",
    "\n",
    "        # case when token starts with whitespace\n",
    "        if text[start_idx].isspace():\n",
    "            start_idx += 1\n",
    "\n",
    "        token_labels.append(label2id[labels[start_idx]])\n",
    "\n",
    "    length = len(tokenized.input_ids)\n",
    "\n",
    "    return {**tokenized, \"labels\": token_labels, \"length\": length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800e99f3199142f483f43e26c875e0dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/9162 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the data utilizing deberta tokenizer\n",
    "TRAIN_MODEL_PATH = \"microsoft/deberta-base\"\n",
    "TRAIN_MAX_LENGTH = 1024\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TRAIN_MODEL_PATH)\n",
    "\n",
    "ds = Dataset.from_dict({\n",
    "    \"full_text\": [x[\"full_text\"] for x in data],\n",
    "    \"document\": [str(x[\"document\"]) for x in data],\n",
    "    \"tokens\": [x[\"tokens\"] for x in data],\n",
    "    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n",
    "    \"provided_labels\": [x[\"labels\"] for x in data],\n",
    "})\n",
    "\n",
    "ds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id, \"max_length\": TRAIN_MAX_LENGTH}, num_proc=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def downsample(df, percent):\n",
    "#     df = df.copy()\n",
    "\n",
    "#     df['is_labels'] = df['provided_labels'].apply(lambda labels: any(label != \"O\" for label in labels))\n",
    "#     true_samples = df[df['is_labels'] == True]\n",
    "#     false_samples = df[df['is_labels'] == False]\n",
    "\n",
    "#     downsampled_false_samples = false_samples.sample(frac=percent, random_state=42)\n",
    "\n",
    "\n",
    "#     return pd.concat([true_samples, downsampled_false_samples])\n",
    "\n",
    "# # Downsample the negative samples of the dataset\n",
    "# df_train = pd.DataFrame(ds)\n",
    "# df_train = downsample(df_train, 0.2)\n",
    "# df_train = df_train.drop(columns=['is_labels'])\n",
    "\n",
    "# ds = Dataset.from_pandas(df_train)\n",
    "# # Splitting the dataset into training and validation sets for performance evaluation\n",
    "# ds = ds.train_test_split(test_size=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "This is a slightly different procedure. We are implementing a weighted voting approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_tokenize(example, tokenizer, max_length):\n",
    "\n",
    "    # rebuild text from tokens\n",
    "    text = []\n",
    "    token_map = []\n",
    "    \n",
    "    idx = 0\n",
    "    \n",
    "    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
    "        text.append(t)\n",
    "        token_map.extend([idx]*len(t))\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            token_map.append(-1)\n",
    "        idx += 1\n",
    "\n",
    "    # actual tokenization\n",
    "    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=max_length)\n",
    "\n",
    "    return {**tokenized, \"token_map\": token_map}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = {\"deberta3base_1024_fold_0\": 0.9750033522765696, \n",
    "               \"deberta3base_1024_fold_1\": 0.8240018195303137, \n",
    "               \"deberta3base_1024_fold_2\": 0.7366411375074039, \n",
    "               \"deberta3base_1024_fold_3\": 0.01050751452921163, \n",
    "               \"deberta3base_1024_fold_4\": 0.01885993245619963}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble method prediction\n",
    "def ensemble_predict(model_paths, dataset):\n",
    "    all_preds = []\n",
    "\n",
    "    for path, weight in model_paths.items():\n",
    "        model = AutoModelForTokenClassification.from_pretrained(path)\n",
    "\n",
    "        # if torch.backends.mps.is_available():\n",
    "        #     mps_device = torch.device(\"mps\")\n",
    "        #     model.to(mps_device)\n",
    "        #     print(\"Model moved to MPS device.\")\n",
    "        # elif torch.cuda.is_available():\n",
    "        #     model.cuda()\n",
    "        #     print(\"Model moved to CUDA device.\")\n",
    "        # else:\n",
    "        #     print(\"No GPU available, using CPU.\")\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "        collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n",
    "\n",
    "        args = TrainingArguments(\n",
    "            \".\",\n",
    "            per_device_eval_batch_size=1,\n",
    "            report_to=\"none\",\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            args,\n",
    "            data_collator=collator,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "\n",
    "        predictions = trainer.predict(dataset).predictions\n",
    "        weighted_predictions = softmax(predictions, axis=-1)\n",
    "        print(type(weighted_predictions))\n",
    "        print(weight)\n",
    "        weighted_predictions = weighted_predictions * weight\n",
    "        all_preds.append(weighted_predictions)\n",
    "\n",
    "    weighted_average_preds = np.sum(all_preds, axis=0) / sum(model_paths.values())\n",
    "\n",
    "    return weighted_average_preds\n",
    "\n",
    "\n",
    "def predict(model, dataset):\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        \".\",\n",
    "        per_device_eval_batch_size=1,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        data_collator=collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    predictions = trainer.predict(dataset).predictions\n",
    "    weighted_predictions = softmax(predictions, axis=-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a66e1337324147a9aab9de232dff5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoniorevilla/Desktop/USC/CSCI567/PII_group_project/env/lib/python3.12/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a123b616c93b4427bf88b3d0231ac89c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11871d8c31674dc1a1af8e31d4ec4904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8481762fbd654215bb9a1a34754cbbe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0448e7c2eb54450a68e9ac3decfb087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525bbe6666904ddfa9d6ae66a1189b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "# TODO: insert test data\n",
    "test_data = json.load(open(\"kaggle/input/pii-detection-removal-from-educational-data/test.json\"))\n",
    "test_frame = pd.DataFrame(test_data)\n",
    "ds_test = Dataset.from_pandas(test_frame)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deberta3base_1024_fold_0\")\n",
    "ds_test = ds_test.map(inference_tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": 2048}, num_proc=3)\n",
    "\n",
    "model_path = \"deberta3base_1024_fold_0\"\n",
    "threshold = 0.99\n",
    "weighted_average_predictions = ensemble_predict(model_paths, ds_test)\n",
    "\n",
    "config = json.load(open(Path(model_path) / \"config.json\"))\n",
    "id2label = config[\"id2label\"]\n",
    "preds = weighted_average_predictions.argmax(-1)\n",
    "preds_without_O = weighted_average_predictions[:,:,:12].argmax(-1)\n",
    "O_preds = weighted_average_predictions[:,:,12]\n",
    "preds_final = np.where(O_preds < threshold, preds_without_O , preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "      <th>token_str</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>reflexion</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>Avril</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document  token           label  token_str  row_id\n",
       "0         7      4  B-NAME_STUDENT  reflexion       0\n",
       "1         7      5  B-NAME_STUDENT          -       1\n",
       "2         7      6  I-NAME_STUDENT      Avril       2\n",
       "3         7      7  I-NAME_STUDENT       2021       3\n",
       "4         7      8  I-NAME_STUDENT          -       4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets = []\n",
    "document, token, label, token_str = [], [], [], []\n",
    "for p, token_map, offsets, tokens, doc in zip(preds_final, ds_test[\"token_map\"], ds_test[\"offset_mapping\"], ds_test[\"tokens\"], ds_test[\"document\"]):\n",
    "\n",
    "    for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
    "        label_pred = id2label[str(token_pred)]\n",
    "\n",
    "        if start_idx + end_idx == 0: continue\n",
    "\n",
    "        if token_map[start_idx] == -1:\n",
    "            start_idx += 1\n",
    "\n",
    "        # ignore \"\\n\\n\"\n",
    "        while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "            start_idx += 1\n",
    "\n",
    "        if start_idx >= len(token_map): break\n",
    "\n",
    "        token_id = token_map[start_idx]\n",
    "\n",
    "        # ignore \"O\" predictions and whitespace preds\n",
    "        if label_pred != \"O\" and token_id != -1:\n",
    "            triplet = (label_pred, token_id, tokens[token_id])\n",
    "\n",
    "            if triplet not in triplets:\n",
    "                document.append(doc)\n",
    "                token.append(token_id)\n",
    "                label.append(label_pred)\n",
    "                token_str.append(tokens[token_id])\n",
    "                triplets.append(triplet)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"document\": document,\n",
    "    \"token\": token,\n",
    "    \"label\": label,\n",
    "    \"token_str\": token_str\n",
    "})\n",
    "\n",
    "df[\"row_id\"] = list(range(len(df)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
